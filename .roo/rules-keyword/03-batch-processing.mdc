---
description: 
globs: 
alwaysApply: false
---
globs:
  - "raw/keywords-paste/**/*.*"
  - "raw/rakko/**/*.txt"
  - "raw/rakko/**/*.csv"
alwaysApply: false
---

# Keyword Batch Processing Rule（03）

## 🎯 目的
大規模なキーワードリストを効率的に処理するため、入力データの規模に応じて動的にバッチサイズを調整し、順次処理を行います。

## 📊 バッチ処理仕様

### 1. 動的分割基準
- **入力規模に応じた自動調整**
  - 1-300行: 単一ファイルとして処理
  - 301-1000行: 300行/バッチで分割
  - 1001行以上: 200行/バッチで分割
  - エラー発生時: 現在のバッチサイズから50%縮小

- **バッチファイル管理**
  - 命名規則: `batch_{timestamp}_{number}.txt`
  - 保存先: `raw/keywords-paste/batches/`
  - ログ保存: `raw/keywords-paste/batches/process.log`

### 2. 前処理チェック
1. **既存データの確認**
   - 処理済みの一時ファイルが存在する場合は削除
   - `temp`ディレクトリをクリーンアップ
   - 処理ログの初期化

2. **入力データの検証**
   - ファイルサイズと行数の確認
   - 文字コードとフォーマットの検証
   - 重複行の事前チェック
   - 最適バッチサイズの算出

### 3. 処理フロー
1. **前処理とクリーンアップ**
   - 新規タスク開始前の必須手順
   ```bash
   # クリーンアップの実行
   ./scripts/cleanup-batch.sh
   
   # 必要なディレクトリの作成
   mkdir -p raw/keywords-paste/batches
   mkdir -p content/keywords/blog/temp
   ```
   
   **確認項目**:
   - 古いバッチファイルの有無
   - 一時ファイルの有無
   - 作業ディレクトリの準備
   - 前回のログファイルの処理

2. **初期分割**
   - 入力ファイルを200行ごとに分割
   - 各バッチファイルを一時保存
   - 分割ログの作成
   - 処理予定時間の見積もり

3. **バッチ処理**
   - 各バッチファイルに対して01-raw-importルールを適用
   - 処理状況をリアルタイムでログに記録
   - エラー発生時の自動リカバリー
     - バッチサイズの50%縮小
     - 最大3回までリトライ
     - 継続的なエラーの場合は手動確認要求

4. **結果統合**
   - 正常処理完了したバッチの結果を統合
   - 重複排除・ランク付けを再実行
   - 最終結果を`content/keywords/blog/keywords.md`に出力
   - 処理サマリーレポートの生成

5. **クリーンアップ処理**
   - バッチ処理完了後の必須タスク
   - 実行タイミング：
     - 全バッチの処理完了後
     - エラーによる中断時
     - 新規タスク開始前
   
   **クリーンアップ対象**:
   ```bash
   # バッチ処理用ディレクトリ
   raw/keywords-paste/batches/*.txt
   raw/keywords-paste/batches/*.log
   
   # 一時ファイル
   content/keywords/blog/temp/*
   
   # 処理ログ
   raw/keywords-paste/batches/process.log
   ```
   
   **実行手順**:
   1. 最終結果のバックアップ確認
   2. 一時ファイルの削除
   3. バッチファイルの削除
   4. ログファイルのアーカイブ（必要な場合）
   
   **エラー時の対応**:
   - クリーンアップ前にバックアップを確保
   - エラーログは削除前にアーカイブ
   - 未処理バッチの状態を記録

### 4. 進捗管理
- **ステータス監視**
  - 総行数
  - 現在のバッチサイズ
  - 処理済みバッチ数
  - 残りバッチ数
  - エラー発生回数
  - 推定残り時間

- **ログ記録**
  - タイムスタンプ
  - 処理状態
  - エラー内容
  - バッチサイズの変更履歴
  - 再試行回数

### 5. エラーハンドリング
1. **自動リカバリー**
   - バッチサイズの動的縮小
   - 処理済みデータのバックアップ
   - 未処理分の再キュー

2. **手動介入要求**
   - 3回連続でエラーが発生
   - 重大なフォーマットエラー
   - 予期せぬ例外発生

### 6. 出力形式
```markdown
# {{製品名}} キーワード分析

## Primary Keyword
{{メインキーワード}} ({{検索ボリューム}})

## Secondary Keywords（重要度順）

### 基本情報・使い方
- {{キーワード1}}
- {{キーワード2}}
...

### 料金・プラン
- {{キーワード1}}
- {{キーワード2}}
...

### アプリ・機能
- {{キーワード1}}
- {{キーワード2}}
...

### セキュリティ
- {{キーワード1}}
- {{キーワード2}}
...
```

## 📊 処理レポート
処理完了後、以下の情報を含むレポートを生成：
- 処理開始・終了時刻
- 総処理時間
- 入力データ総行数
- 最終バッチサイズ
- 総バッチ数
- エラー発生回数
- リカバリー成功率
- 最終出力キーワード数

## ⚠️ 注意事項
- バッチサイズは入力データの規模に応じて自動調整されます
- エラー発生時は自動的にバッチサイズが縮小されます
- 処理結果は必ずバックアップを取ってください
- 大規模データの場合は処理時間が長くなる可能性があります

## 🔄 統合ルール

### 1. キーワードランク付け
- 全バッチの検索ボリュームを統合して再計算
- Primary: 最高検索ボリュームの1キーワード
- Secondary: カテゴリごとに5-10件を選定
- Support: カテゴリごとに5-15件を選定

### 2. 重複排除基準
- 完全一致の重複を除去
- 類似度90%以上のキーワードをマージ
- 検索ボリュームの高い方を採用

## ✅ 完了チェックリスト
- [ ] すべてのバッチの処理状態を確認
  - 正常処理完了
  - エラーでスキップ
  - 未処理
- [ ] 一時ファイルの確認
  - 処理済みバッチの削除
  - ログファイルの保存
- [ ] 最終出力の品質確認
  - フォーマット検証
  - カテゴリ分類の確認
  - キーワード数の確認

---

> **Note:** 
> - バッチ処理の再実行時は必ず前回の一時ファイルをクリーンアップしてから開始
> - Claude 3.7 Sonnet以外のモデルを使用する場合は、バッチサイズを200行以下に制限
> - エラー発生時は処理状況を保存し、再開可能な状態を維持
